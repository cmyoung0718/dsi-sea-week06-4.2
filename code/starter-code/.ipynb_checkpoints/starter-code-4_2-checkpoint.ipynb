{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Lab\n",
    "\n",
    "In this lab we will further explore Scikit's and NLTK's capabilities to process text. We will use the 20 Newsgroup dataset, which is provided by Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories,\n",
    "                               shuffle=True, random_state=42,\n",
    "                               remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data inspection\n",
    "\n",
    "We have downloaded a few newsgroup categories and removed headers, footers and quotes.\n",
    "\n",
    "Let's inspect them.\n",
    "\n",
    "1. What data taype is `data_train`\n",
    "> sklearn.datasets.base.Bunch\n",
    "- Is it like a list? Or like a Dictionary? or what?\n",
    "> Dict\n",
    "- How many data points does it contain?\n",
    "- Inspect the first data point, what does it look like?\n",
    "> A blurb of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2034 data points in it.\n"
     ]
    }
   ],
   "source": [
    "print 'There are', len(data_train.data), 'data points in it.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag of Words model\n",
    "\n",
    "Let's train a model using a simple count vectorizer\n",
    "\n",
    "1. Initialize a standard CountVectorizer and fit the training data\n",
    "- how big is the feature dictionary\n",
    "- repeat eliminating english stop words\n",
    "- is the dictionary smaller?\n",
    "- transform the training data using the trained vectorizer\n",
    "- what are the 20 words that are most common in the whole corpus?\n",
    "- what are the 20 most common words in each of the 4 classes?\n",
    "- evaluate the performance of a Lotistic Regression on the features extracted by the CountVectorizer\n",
    "    - you will have to transform the test_set too. Be carefule to use the trained vectorizer, without re-fitting it\n",
    "- try the following 3 modification:\n",
    "    - restrict the max_features\n",
    "    - change max_df and min_df\n",
    "    - use a fixed vocabulary of size 80 combining the 20 most common words per group found earlier\n",
    "- for each of the above print a confusion matrix and investigate what gets mixed\n",
    "> Anwer: not surprisingly if we reduce the feature space we lose accuracy\n",
    "- print out the number of features for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize a standard CountVectorizer and fit the training data\n",
    "v = CountVectorizer()\n",
    "X_train_counts = v.fit_transform(data_train.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2034, 26879)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how big is the feature dictionary\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# repeat eliminating english stop words\n",
    "v = CountVectorizer(stop_words='english')\n",
    "X_train_counts = v.fit_transform(data_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2034, 26576)\n",
      "It is barely smaller.\n"
     ]
    }
   ],
   "source": [
    "# is the dictionary smaller?\n",
    "print X_train_counts.shape\n",
    "print 'It is barely smaller.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform the training data using the trained vectorizer\n",
    "v = CountVectorizer(stop_words='english')\n",
    "X_vec = v.fit_transform(data_train.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'data': 0,\n",
       " u'does': 1,\n",
       " u'don': 2,\n",
       " u'edu': 3,\n",
       " u'god': 4,\n",
       " u'good': 5,\n",
       " u'graphics': 6,\n",
       " u'image': 7,\n",
       " u'jesus': 8,\n",
       " u'just': 9,\n",
       " u'know': 10,\n",
       " u'like': 11,\n",
       " u'nasa': 12,\n",
       " u'people': 13,\n",
       " u'say': 14,\n",
       " u'space': 15,\n",
       " u'think': 16,\n",
       " u'time': 17,\n",
       " u'use': 18,\n",
       " u'way': 19}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what are the 20 words that are most common in the whole corpus?\n",
    "_20 = CountVectorizer(stop_words='english', max_features = 20)\n",
    "_20.fit_transform(data_train.data)\n",
    "_20.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi,\\n\\nI've noticed that if you only save a mo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\nSeems to be, barring evidence to the contr...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n &gt;In article &lt;1993Apr19.020359.26996@sq.sq.c...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I have a request for those who would like to s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AW&amp;ST  had a brief blurb on a Manned Lunar Exp...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  class\n",
       "0  Hi,\\n\\nI've noticed that if you only save a mo...      1\n",
       "1  \\n\\nSeems to be, barring evidence to the contr...      3\n",
       "2  \\n >In article <1993Apr19.020359.26996@sq.sq.c...      2\n",
       "3  I have a request for those who would like to s...      0\n",
       "4  AW&ST  had a brief blurb on a Manned Lunar Exp...      2"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(data_train.data, columns =['data'])\n",
    "y = pd.DataFrame(data_train.target, columns =['class'])\n",
    "y.head()\n",
    "stuff = pd.concat([X, y], axis = 1)\n",
    "stuff.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [u'people', u'time', u'argument', u'say', u'religion', u'atheists', u'don', u'jesus', u'does', u'way', u'true', u'atheism', u'said', u'just', u'think', u'bible', u'like', u'god', u'believe', u'know']\n",
      "1 [u'format', u'data', u'image', u'gif', u'ftp', u'graphics', u'does', u'software', u'available', u'use', u'pub', u'like', u'images', u'file', u'edu', u'jpeg', u'color', u'program', u'files', u'know']\n",
      "2 [u'people', u'time', u'data', u'just', u'year', u'space', u'launch', u'orbit', u'new', u'don', u'lunar', u'shuttle', u'like', u'earth', u'satellite', u'moon', u'program', u'mission', u'nasa', u'use']\n",
      "3 [u'life', u'people', u'time', u'say', u'jesus', u'does', u'way', u'think', u'don', u'said', u'just', u'did', u'bible', u'good', u'believe', u'point', u'like', u'god', u'christian', u'know']\n"
     ]
    }
   ],
   "source": [
    "# what are the 20 most common words in each of the 4 classes?\n",
    "lsity = []\n",
    "for i in range(4):\n",
    "    _20 = CountVectorizer(stop_words='english', max_features = 20)\n",
    "    _20.fit_transform(stuff.ix[stuff['class']== i]['data'])\n",
    "    print i, [x for x in _20.vocabulary_]\n",
    "    listy.append([x for x in _20.vocabulary_])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listy2 = []\n",
    "for i in listy:\n",
    "    for j in i:\n",
    "        listy2.append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this = 0\n",
    "# _20 = CountVectorizer(stop_words='english', max_features = 20)\n",
    "# for i in range(4):\n",
    "#     if this == 0:\n",
    "#         _80thing = pd.DataFrame(_20.fit_transform(stuff.ix[stuff['class']== i]['data']).todense())\n",
    "#         this =1\n",
    "#     elif this == 1:\n",
    "#         print this\n",
    "#         _80thing = pd.concat([_80thing, pd.DataFrame(_20.fit_transform(stuff.ix[stuff['class']== i]['data']).todense())], axis=1)\n",
    "#         this = 2\n",
    "#     elif this == 2:\n",
    "#         print this\n",
    "#         this = 3\n",
    "#         _80thing = pd.concat([_80thing, pd.DataFrame(_20.fit_transform(stuff.ix[stuff['class']== i]['data']).todense())], axis=1)\n",
    "#     else:\n",
    "#         print this\n",
    "#         _80thing = pd.concat([_80thing, pd.DataFrame(_20.fit_transform(stuff.ix[stuff['class']== i]['data']).todense())], axis=1)\n",
    "# _80thing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.745011086475\n",
      "con mat: \n",
      "[[187  16  46  70]\n",
      " [ 13 345  28   3]\n",
      " [ 22  23 333  16]\n",
      " [ 67  14  27 143]]\n",
      "number of words: 26576\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "evaluate the performance of a Lotistic Regression on the features extracted by the CountVectorizer\n",
    "you will have to transform the test_set too. Be carefule to use the trained vectorizer, without re-fitting it\n",
    "'''\n",
    "def log_score(v):\n",
    "    X_vec = v.fit_transform(data_train.data)\n",
    "    test_vec = v.transform(data_test.data)\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_vec, data_train.target)\n",
    "    y_pred = lr.predict(test_vec)\n",
    "    print 'score:', lr.score(test_vec, data_test.target)\n",
    "    print  'con mat:','\\n', confusion_matrix(data_test.target, y_pred)\n",
    "    print 'number of words:', len(v.vocabulary_)\n",
    "    \n",
    "v = CountVectorizer(stop_words='english')\n",
    "log_score(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features = 10\n",
      "score: 0.427198817443\n",
      "con mat: \n",
      "[[108 160  37  14]\n",
      " [ 23 310  55   1]\n",
      " [ 31 214 148   1]\n",
      " [ 89 129  21  12]]\n",
      "number of words: 10\n",
      "\n",
      "\n",
      "max_features = 100\n",
      "score: 0.619364375462\n",
      "con mat: \n",
      "[[161  28  56  74]\n",
      " [ 19 302  59   9]\n",
      " [ 38  34 298  24]\n",
      " [107  19  48  77]]\n",
      "number of words: 100\n",
      "\n",
      "\n",
      "max_features = 1000\n",
      "score: 0.69696969697\n",
      "con mat: \n",
      "[[172  15  59  73]\n",
      " [ 16 331  36   6]\n",
      " [ 28  29 312  25]\n",
      " [ 76  16  31 128]]\n",
      "number of words: 1000\n",
      "\n",
      "\n",
      "max_features = 10000\n",
      "score: 0.742054693274\n",
      "con mat: \n",
      "[[186  15  46  72]\n",
      " [ 14 344  28   3]\n",
      " [ 21  27 331  15]\n",
      " [ 67  14  27 143]]\n",
      "number of words: 10000\n",
      "\n",
      "\n",
      "max_df = 0.1\n",
      "score: 0.746489283075\n",
      "con mat: \n",
      "[[186  16  52  65]\n",
      " [ 11 341  34   3]\n",
      " [ 21  27 335  11]\n",
      " [ 59  14  30 148]]\n",
      "number of words: 26558\n",
      "\n",
      "\n",
      "max_df = 0.2\n",
      "score: 0.747967479675\n",
      "con mat: \n",
      "[[184  15  48  72]\n",
      " [  9 345  32   3]\n",
      " [ 19  25 336  14]\n",
      " [ 64  16  24 147]]\n",
      "number of words: 26572\n",
      "\n",
      "\n",
      "max_df = 0.3\n",
      "score: 0.745011086475\n",
      "con mat: \n",
      "[[187  16  46  70]\n",
      " [ 13 345  28   3]\n",
      " [ 22  23 333  16]\n",
      " [ 67  14  27 143]]\n",
      "number of words: 26576\n",
      "\n",
      "\n",
      "max_df = 0.4\n",
      "score: 0.745011086475\n",
      "con mat: \n",
      "[[187  16  46  70]\n",
      " [ 13 345  28   3]\n",
      " [ 22  23 333  16]\n",
      " [ 67  14  27 143]]\n",
      "number of words: 26576\n",
      "\n",
      "\n",
      "max_df = 0.5\n",
      "score: 0.745011086475\n",
      "con mat: \n",
      "[[187  16  46  70]\n",
      " [ 13 345  28   3]\n",
      " [ 22  23 333  16]\n",
      " [ 67  14  27 143]]\n",
      "number of words: 26576\n",
      "\n",
      "\n",
      "max_df = 0.6\n",
      "score: 0.745011086475\n",
      "con mat: \n",
      "[[187  16  46  70]\n",
      " [ 13 345  28   3]\n",
      " [ 22  23 333  16]\n",
      " [ 67  14  27 143]]\n",
      "number of words: 26576\n",
      "\n",
      "\n",
      "max_df = 0.7\n",
      "score: 0.745011086475\n",
      "con mat: \n",
      "[[187  16  46  70]\n",
      " [ 13 345  28   3]\n",
      " [ 22  23 333  16]\n",
      " [ 67  14  27 143]]\n",
      "number of words: 26576\n",
      "\n",
      "\n",
      "max_df = 0.8\n",
      "score: 0.745011086475\n",
      "con mat: \n",
      "[[187  16  46  70]\n",
      " [ 13 345  28   3]\n",
      " [ 22  23 333  16]\n",
      " [ 67  14  27 143]]\n",
      "number of words: 26576\n",
      "\n",
      "\n",
      "max_df = 0.9\n",
      "score: 0.745011086475\n",
      "con mat: \n",
      "[[187  16  46  70]\n",
      " [ 13 345  28   3]\n",
      " [ 22  23 333  16]\n",
      " [ 67  14  27 143]]\n",
      "number of words: 26576\n",
      "\n",
      "\n",
      "min_df = 0.1\n",
      "score: 0.441241685144\n",
      "con mat: \n",
      "[[116 125  41  37]\n",
      " [ 30 287  67   5]\n",
      " [ 39 181 170   4]\n",
      " [ 88 112  27  24]]\n",
      "number of words: 18\n",
      "\n",
      "\n",
      "min_df = 0.2\n",
      "score: 0.305247597931\n",
      "con mat: \n",
      "[[ 42 205  70   2]\n",
      " [ 23 264 101   1]\n",
      " [ 31 257 105   1]\n",
      " [ 25 180  44   2]]\n",
      "number of words: 4\n",
      "\n",
      "\n",
      "With 80 most common words from each class.\n",
      "score: 0.589061345159\n",
      "con mat: \n",
      "[[160  67  33  59]\n",
      " [ 25 314  44   6]\n",
      " [ 37  88 247  22]\n",
      " [ 90  70  15  76]]\n",
      "number of words: 54\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "try the following 3 modification:\n",
    "restrict the max_features\n",
    "change max_df and min_df\n",
    "use a fixed vocabulary of size 80 combining the 20 most common words per group found earlier\n",
    "for each of the above print a confusion matrix and investigate what gets mixed\n",
    "Anwer: not surprisingly if we reduce the feature space we lose accuracy\n",
    "print out the number of features for each model\n",
    "'''\n",
    "\n",
    "for i in [10, 100, 1000, 10000]:\n",
    "    print 'max_features =', i\n",
    "    v = CountVectorizer(stop_words='english', max_features = i)\n",
    "    log_score(v)\n",
    "    print '\\n'\n",
    "    \n",
    "for i in [.1, .2, .3, .4, .5, .6, .7, .8, .9]:\n",
    "    print 'max_df =', i\n",
    "    v = CountVectorizer(stop_words='english', max_df = i)\n",
    "    log_score(v)\n",
    "    print '\\n'\n",
    "    \n",
    "for i in [.1, .2]:\n",
    "    print 'min_df =', i\n",
    "    v = CountVectorizer(stop_words='english', min_df = i)\n",
    "    log_score(v)\n",
    "    print '\\n'\n",
    "\n",
    "print 'With 80 most common words from each class.'\n",
    "v = CountVectorizer(stop_words='english', vocabulary=list(set(listy2)))\n",
    "log_score(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hashing and TF-IDF\n",
    "\n",
    "Let's see if Hashing or TF-IDF improves the accuracy.\n",
    "\n",
    "1. Initialize a HashingVectorizer and repeat the test with no restriction on the number of features\n",
    "- does the score improve with respect to the count vectorizer?\n",
    "    - can you change any of the default parameters to improve it?\n",
    "- print out the number of features for this model\n",
    "- Initialize a TF-IDF Vectorizer and repeat the analysis above\n",
    "- can you improve on your best score above?\n",
    "    - can you change any of the default parameters to improve it?\n",
    "- print out the number of features for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_score_2(v):\n",
    "    X_vec = v.fit_transform(data_train.data)\n",
    "    test_vec = v.transform(data_test.data)\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_vec, data_train.target)\n",
    "    y_pred = lr.predict(test_vec)\n",
    "    print 'score:', lr.score(test_vec, data_test.target)\n",
    "    print  'con mat:','\\n', confusion_matrix(data_test.target, y_pred)\n",
    "    print 'number of features:', v.n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.736881005174\n",
      "con mat: \n",
      "[[197  15  65  42]\n",
      " [  9 347  32   1]\n",
      " [ 21  23 350   0]\n",
      " [ 86  18  44 103]]\n",
      "number of features: 1048576\n"
     ]
    }
   ],
   "source": [
    "v = HashingVectorizer(stop_words='english')\n",
    "log_score_2(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.747967479675\n",
      "con mat: \n",
      "[[198  15  65  41]\n",
      " [  8 351  29   1]\n",
      " [ 17  21 356   0]\n",
      " [ 82  16  46 107]]\n",
      "number of words: 26576\n",
      "max_features = 10\n",
      "score: 0.430155210643\n",
      "con mat: \n",
      "[[150 141  24   4]\n",
      " [ 49 292  48   0]\n",
      " [ 54 201 139   0]\n",
      " [112 123  15   1]]\n",
      "number of words: 10\n",
      "\n",
      "\n",
      "max_features = 100\n",
      "score: 0.631189948263\n",
      "con mat: \n",
      "[[168  26  64  61]\n",
      " [ 21 306  59   3]\n",
      " [ 35  38 304  17]\n",
      " [109  21  45  76]]\n",
      "number of words: 100\n",
      "\n",
      "\n",
      "max_features = 1000\n",
      "score: 0.726533628973\n",
      "con mat: \n",
      "[[185  16  63  55]\n",
      " [  7 343  37   2]\n",
      " [ 23  26 337   8]\n",
      " [ 79  15  39 118]]\n",
      "number of words: 1000\n",
      "\n",
      "\n",
      "max_features = 10000\n",
      "score: 0.748706577975\n",
      "con mat: \n",
      "[[198  17  60  44]\n",
      " [  8 350  30   1]\n",
      " [ 17  22 355   0]\n",
      " [ 83  18  40 110]]\n",
      "number of words: 10000\n",
      "\n",
      "\n",
      "max_df = 0.1\n",
      "score: 0.739837398374\n",
      "con mat: \n",
      "[[191  11  76  41]\n",
      " [  7 345  36   1]\n",
      " [ 12  23 359   0]\n",
      " [ 76  13  56 106]]\n",
      "number of words: 26558\n",
      "\n",
      "\n",
      "max_df = 0.2\n",
      "score: 0.749445676275\n",
      "con mat: \n",
      "[[194  14  66  45]\n",
      " [  7 351  30   1]\n",
      " [ 13  22 359   0]\n",
      " [ 79  16  46 110]]\n",
      "number of words: 26572\n",
      "\n",
      "\n",
      "max_df = 0.3\n",
      "score: 0.747967479675\n",
      "con mat: \n",
      "[[198  15  65  41]\n",
      " [  8 351  29   1]\n",
      " [ 17  21 356   0]\n",
      " [ 82  16  46 107]]\n",
      "number of words: 26576\n",
      "\n",
      "\n",
      "max_df = 0.4\n",
      "score: 0.747967479675\n",
      "con mat: \n",
      "[[198  15  65  41]\n",
      " [  8 351  29   1]\n",
      " [ 17  21 356   0]\n",
      " [ 82  16  46 107]]\n",
      "number of words: 26576\n",
      "\n",
      "\n",
      "max_df = 0.5\n",
      "score: 0.747967479675\n",
      "con mat: \n",
      "[[198  15  65  41]\n",
      " [  8 351  29   1]\n",
      " [ 17  21 356   0]\n",
      " [ 82  16  46 107]]\n",
      "number of words: 26576\n",
      "\n",
      "\n",
      "max_df = 0.6\n",
      "score: 0.747967479675\n",
      "con mat: \n",
      "[[198  15  65  41]\n",
      " [  8 351  29   1]\n",
      " [ 17  21 356   0]\n",
      " [ 82  16  46 107]]\n",
      "number of words: 26576\n",
      "\n",
      "\n",
      "max_df = 0.7\n",
      "score: 0.747967479675\n",
      "con mat: \n",
      "[[198  15  65  41]\n",
      " [  8 351  29   1]\n",
      " [ 17  21 356   0]\n",
      " [ 82  16  46 107]]\n",
      "number of words: 26576\n",
      "\n",
      "\n",
      "max_df = 0.8\n",
      "score: 0.747967479675\n",
      "con mat: \n",
      "[[198  15  65  41]\n",
      " [  8 351  29   1]\n",
      " [ 17  21 356   0]\n",
      " [ 82  16  46 107]]\n",
      "number of words: 26576\n",
      "\n",
      "\n",
      "max_df = 0.9\n",
      "score: 0.747967479675\n",
      "con mat: \n",
      "[[198  15  65  41]\n",
      " [  8 351  29   1]\n",
      " [ 17  21 356   0]\n",
      " [ 82  16  46 107]]\n",
      "number of words: 26576\n",
      "\n",
      "\n",
      "min_df = 0.1\n",
      "score: 0.449371766445\n",
      "con mat: \n",
      "[[141 125  37  16]\n",
      " [ 37 280  72   0]\n",
      " [ 48 172 173   1]\n",
      " [105 108  24  14]]\n",
      "number of words: 18\n",
      "\n",
      "\n",
      "min_df = 0.2\n",
      "score: 0.311160384331\n",
      "con mat: \n",
      "[[ 71 174  74   0]\n",
      " [ 43 241 105   0]\n",
      " [ 57 228 109   0]\n",
      " [ 43 161  47   0]]\n",
      "number of words: 4\n",
      "\n",
      "\n",
      "With 80 most common words from each class.\n",
      "score: 0.60310421286\n",
      "con mat: \n",
      "[[166  62  29  62]\n",
      " [ 24 321  42   2]\n",
      " [ 39  87 253  15]\n",
      " [ 94  65  16  76]]\n",
      "number of words: 54\n",
      "Reducing max_df to 0.2 increased the score.\n"
     ]
    }
   ],
   "source": [
    "v = TfidfVectorizer(stop_words='english')\n",
    "log_score(v)\n",
    "\n",
    "for i in [10, 100, 1000, 10000]:\n",
    "    print 'max_features =', i\n",
    "    v = TfidfVectorizer(stop_words='english', max_features = i)\n",
    "    log_score(v)\n",
    "    print '\\n'\n",
    "    \n",
    "for i in [.1, .2, .3, .4, .5, .6, .7, .8, .9]:\n",
    "    print 'max_df =', i\n",
    "    v = TfidfVectorizer(stop_words='english', max_df = i)\n",
    "    log_score(v)\n",
    "    print '\\n'\n",
    "    \n",
    "for i in [.1, .2]:\n",
    "    print 'min_df =', i\n",
    "    v = TfidfVectorizer(stop_words='english', min_df = i)\n",
    "    log_score(v)\n",
    "    print '\\n'\n",
    "\n",
    "print 'With 80 most common words from each class.'\n",
    "v = TfidfVectorizer(stop_words='english', vocabulary=list(set(listy2)))\n",
    "log_score(v)\n",
    "\n",
    "print 'Reducing max_df to 0.2 increased the score.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classifier comparison\n",
    "\n",
    "Of all the vectorizers tested above, choose one that has a reasonable performance with a manageable number of features and compare the performance of these models:\n",
    "\n",
    "- KNN\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- Support Vector Machine\n",
    "- Random Forest\n",
    "- Extra Trees\n",
    "\n",
    "In order to speed up the calculation it's better to vectorize the data only once and then compare the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')\n",
      "score: 0.252032520325\n",
      "con mat: \n",
      "[[122 108  34  55]\n",
      " [158 101  62  68]\n",
      " [118 135  72  69]\n",
      " [ 86  80  39  46]]\n",
      "number of words: 26572\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "score: 0.749445676275\n",
      "con mat: \n",
      "[[194  14  66  45]\n",
      " [  7 351  30   1]\n",
      " [ 13  22 359   0]\n",
      " [ 79  16  46 110]]\n",
      "number of words: 26572\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')\n",
      "score: 0.609016999261\n",
      "con mat: \n",
      "[[143  53  48  75]\n",
      " [ 21 311  45  12]\n",
      " [ 38  72 271  13]\n",
      " [ 77  49  26  99]]\n",
      "number of words: 26572\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "score: 0.291204730229\n",
      "con mat: \n",
      "[[  0   0 319   0]\n",
      " [  0   0 389   0]\n",
      " [  0   0 394   0]\n",
      " [  0   0 251   0]]\n",
      "number of words: 26572\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "score: 0.673318551367\n",
      "con mat: \n",
      "[[191  24  54  50]\n",
      " [ 21 332  34   2]\n",
      " [ 37  41 304  12]\n",
      " [107  25  35  84]]\n",
      "number of words: 26572\n",
      "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
      "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "score: 0.667405764967\n",
      "con mat: \n",
      "[[196  35  43  45]\n",
      " [ 12 347  29   1]\n",
      " [ 40  76 274   4]\n",
      " [111  20  34  86]]\n",
      "number of words: 26572\n"
     ]
    }
   ],
   "source": [
    "models = [KNeighborsClassifier(), LogisticRegression(), DecisionTreeClassifier(), SVC(), RandomForestClassifier(), ExtraTreesClassifier()]\n",
    "\n",
    "def mod_score(model, v):\n",
    "    X_vec = v.fit_transform(data_train.data)\n",
    "    test_vec = v.transform(data_test.data)\n",
    "    print model\n",
    "    mod = model\n",
    "    mod.fit(X_vec, data_train.target)\n",
    "    y_pred = mod.predict(test_vec)\n",
    "    print 'score:', mod.score(test_vec, data_test.target)\n",
    "    print  'con mat:','\\n', confusion_matrix(data_test.target, y_pred)\n",
    "    print 'number of words:', len(v.vocabulary_)\n",
    "v = TfidfVectorizer(stop_words='english', max_df=0.2)\n",
    "for i in models:\n",
    "    mod_score(i, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Other classifiers\n",
    "\n",
    "Adapt the code from [this example](http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#example-text-document-classification-20newsgroups-py) to compare across all the classifiers suggested and to display the final plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: NLTK\n",
    "\n",
    "NLTK is a vast library. Can you find some interesting bits to share with classmates?\n",
    "Start here: http://www.nltk.org/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
